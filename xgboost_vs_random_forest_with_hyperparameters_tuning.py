# -*- coding: utf-8 -*-
"""XGboost vs Random forest with hyperparameters tuning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tzmneZKS-ZXcLfqKoAh4OPYE3MxZHH6N

# **#XGboost vs Random forest with hyperparameters tuning**
"""

import pandas as pd

import matplotlib.pyplot as plt

import seaborn as sns

from sklearn.model_selection import train_test_split

from sklearn.ensemble import RandomForestRegressor

from xgboost import XGBRegressor

from sklearn.metrics import mean_squared_error
from sklearn.model_selection import GridSearchCV


# Load the diamonds dataset

diamonds_df = sns.load_dataset('diamonds')

# Split the data into training and testing sets

X = diamonds_df.drop('price', axis=1)

y = diamonds_df['price']

#label encoding

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
X['cut'] = le.fit_transform(X['cut'])

X['color'] = le.fit_transform(X['color']) 

X['clarity'] = le.fit_transform(X['clarity'])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Define the models

rf = RandomForestRegressor(random_state=42)

xgb = XGBRegressor(random_state=42)

models = {'Random Forest': rf, 'XGBoost': xgb}

for name, model in models.items():

     model.fit(X_train, y_train)

     y_pred = model.predict(X_test) 

     mse = mean_squared_error(y_test, y_pred)

# Define the models

rf = RandomForestRegressor(random_state=42) 
xgb = XGBRegressor(random_state=42)

# hyper parameters of these models

rf_params = {
    'n_estimators': [100, 300, 500],
     'max_depth': [10, 20, 30],
     'min_samples_split': [2, 5, 10],
     'min_samples_leaf': [1, 2, 4]

}

xgb_params = {

     'n_estimators': [100, 300, 500],
      'max_depth': [3, 5, 7],
      'learning_rate': [0.1, 0.01, 0.001],
      'subsample': [0.6, 0.8, 1],
      'colsample_bytree': [0.6, 0.8, 1]
}
# Perform grid search to find the best hyperparameters for each model

rf_gs = GridSearchCV(rf, rf_params, scoring='neg_mean_squared_error', cv=5)

rf_gs.fit(X_train, y_train)

rf_best = rf_gs.best_estimator_

xgb_gs = GridSearchCV(xgb, xgb_params, scoring='neg_mean_squared_error', cv=5)
xgb_gs.fit(X_train, y_train)
xgb_best = xgb_gs.best_estimator__


# Evaluate the best models on the testing set

rf_pred = rf_best.predict(X_test)

rf_mse = np.mean ((rf_pred - y_test) ** 2)

print(f'Random Forest - MSE: {rf_mse:.2f}')

xgb_pred = xgb_best.predict(X_test)

xgb_mse = np.mean((xgb_pred - y_test) ** 2) 

print(f'XGBoost - MSE: {xgb_mse:.2f}')